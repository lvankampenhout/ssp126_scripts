{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import matplotlib.pyplot as plt\n",
    "import xarray as xr\n",
    "import datetime\n",
    "import numpy as np\n",
    "import multiprocessing\n",
    "\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.16.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xr.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import dask\n",
    "#dask.config.set(scheduler=\"single-threaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "workspace = \"/glade/work/lvank/for_Melchior/\"\n",
    "\n",
    "# Historical\n",
    "casename = \"b.e21.BHIST.f09_g17.CMIP6-historical.011\"\n",
    "\n",
    "# SSP126\n",
    "#casename = \"b.e21.BSSP126cmip6.f09_g17.CMIP6-SSP1-2.6.102\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(os.path.join(workspace, casename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "!echo $OMP_NUM_THREADS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NPROC = 10\n",
    "NPROC = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2. Functions to read 6-hourly and daily data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_ATM(var_dict):\n",
    "    \"\"\"\n",
    "    6-hourly ATM variables \n",
    "    \n",
    "    no corrections needed on these ones\n",
    "    \"\"\"\n",
    "    ds = xr.open_mfdataset('hus/*.nc', combine='by_coords')\n",
    "    var_dict['hus'] = ds.hus\n",
    "    \n",
    "    ds = xr.open_mfdataset('ta/*.nc', combine='by_coords')\n",
    "    var_dict['ta'] = ds.ta\n",
    "\n",
    "    ds = xr.open_mfdataset('ua/*.nc', combine='by_coords')\n",
    "    var_dict['ua'] = ds.ua\n",
    "    \n",
    "    ds = xr.open_mfdataset('va/*.nc', combine='by_coords')\n",
    "    var_dict['va'] = ds.va\n",
    "    \n",
    "    ds = xr.open_mfdataset('ps/*.nc', combine='by_coords')\n",
    "    var_dict['ps'] = ds.ps\n",
    "    \n",
    "    # NOTE: TAS has daily freq\n",
    "    # will need to be dealt with later!! \n",
    "    ds = xr.open_mfdataset('tas/*.nc', combine='by_coords')\n",
    "    var_dict['tas'] = ds.tas\n",
    "   \n",
    "    #x = [len(var_dict[varname]) == 125561 for varname in ['hus', 'ta', 'ua', 'va', 'ps']]\n",
    "    #assert all(x), x\n",
    "\n",
    "    #x = [len(var_dict[varname]) == 31391 for varname in ['tas']]\n",
    "    #assert all(x), x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_oceanvar(var_dict, varname, lat, lon):\n",
    "    \"\"\"\n",
    "    Read ocean variable\n",
    "    \n",
    "    Ocean variables like TOS have been manually interpolated to the ATM grid using CDO. \n",
    "    The LAT LON have been messed up as a result, need to align with the ATM variables before merging. \n",
    "    \n",
    "    Also: because it is POP/CICE output, the first of January at the beginning of the simulation is missing. \n",
    "    This will be fixed (interpolated) later when we process that particular month. Doing it here would require\n",
    "    a large overhead (xr.concat is very slow on big datasets...)\n",
    "    \"\"\"\n",
    "    ds = xr.open_mfdataset(f'{varname}/fv1_grid/*.nc', combine='by_coords')\n",
    "    \n",
    "    var = ds[varname]\n",
    "    \n",
    "    # WORKAROUND: since were are combining two seperate files, there is a single time step at 2065-01-01\n",
    "    # which should contain 4 time steps. Drop this single time step, we'll re-fill this day later in the script. \n",
    "    \"\"\"\n",
    "    dropme = siconc.sel(time='2065-01-01').time.item()\n",
    "    print('DROPPING ', dropme)\n",
    "    siconc = siconc.drop([dropme], dim='time')\n",
    "    \"\"\"\n",
    "    \n",
    "    del var['lat']\n",
    "    del var['lon']\n",
    "    \n",
    "    var = var.rename({'y':'lat', 'x':'lon'})\n",
    "    \n",
    "    var['lat'] = lat\n",
    "    var['lon'] = lon\n",
    "    \n",
    "\n",
    "    #assert(len(var) == 31390)\n",
    "    var_dict[varname] = var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_var_month(var_dict, mon_str, varname):\n",
    "    \"\"\"\n",
    "    Returns Xarray DataArray with single variable for single month\n",
    "    \"\"\"   \n",
    "    if (varname in ['tas', 'siconc', 'tos']): \n",
    "        # Daily variables, interpolate to 6-hourly\n",
    "        var = get_var_month_from_daily(var_dict, mon_str, varname)\n",
    "    else:\n",
    "        var = var_dict[varname].sel(time=slice(mon_str,mon_str))\n",
    "\n",
    "    l1 = mon_str == '2015-01' and varname == 'tos'\n",
    "    l2 = mon_str == '2015-01' and varname == 'siconc'\n",
    "    l3 = mon_str == '2065-01' and varname == 'siconc'\n",
    "\n",
    "    if (l1 or l2 or l3):\n",
    "        # variable from POP misses first day\n",
    "        # Copy day 2 into day 1\n",
    "        tmp = var[[0,1,2,3]].copy()\n",
    "        x = [y.replace(day=1) for y in tmp.time.data]\n",
    "        x_da = xr.DataArray(x, coords=[x,], dims='time')\n",
    "        tmp['time'] = x_da\n",
    "        #print(tmp)\n",
    "        var = xr.concat([tmp, var], dim='time')\n",
    "        \n",
    "    return var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_var_month_from_daily(var_dict, mon_str, varname):\n",
    "    \"\"\"\n",
    "    Special case: TAS has daily freq, and I had trouble resampling with CDO\n",
    "    so upsample directly here in Python\n",
    "    \n",
    "    UPDATE: do this for all daily variables. Easier. \n",
    "    \"\"\"\n",
    "    \n",
    "    foo = [int(x) for x in mon_str.split('-')] # get Y and M as integer\n",
    "    mydate = datetime.datetime(*foo, 1)\n",
    "    #print(mydate)\n",
    "    date_next = mydate + relativedelta(months=1) # 1 month later\n",
    "    #print(date_next)\n",
    "    mon_str2 = f'{date_next:%Y-%m-%d}' # first day of next month, as a string\n",
    "    \n",
    "    return var_dict[varname].sel(time=slice(mon_str, mon_str2)).resample(time='6H').interpolate('linear').sel(time=mon_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_DataSet_month(var_dict, mon_str):\n",
    "    \"\"\"\n",
    "    Returns Xarray DataSet with all variables for single month\n",
    "    \n",
    "    This is the main function that calls the other functions above. \n",
    "    \"\"\"\n",
    "    var_list = [get_var_month(var_dict, mon_str, x) for x in ['tas', 'ta', 'ua', 'va', 'ps', 'siconc', 'hus', 'tos'] ]\n",
    "    \n",
    "    # Check that all DataArrays have the same length\n",
    "    nt = [len(x) for x in var_list]\n",
    "    assert all(element == nt[0] for element in nt), nt\n",
    "    \n",
    "    ds_out = xr.merge(var_list)\n",
    "    return ds_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_global_attributes(ds):\n",
    "    ds.attrs['description'] = \"6-hourly CESM output for forcing RACMO2 RCM\"\n",
    "    ds.attrs['author'] = \"L.vankampenhout@uu.nl\"\n",
    "    ds.attrs['creation_date'] = f'{datetime.datetime.now():%Y-%m-%d}'\n",
    "    ds.attrs['source_script'] = \"make_monthly.ipynb, archived at https://github.com/lvankampenhout/ssp126_scripts\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## some serial testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# var_dict = {}\n",
    "\n",
    "# read_ATM(var_dict)\n",
    "# lat, lon = var_dict['hus'].lat, var_dict['hus'].lon\n",
    "# read_oceanvar(var_dict, 'siconc', lat, lon)\n",
    "# read_oceanvar(var_dict, 'tos', lat, lon)\n",
    "\n",
    "# ds = get_DataSet_month(var_dict, '1950-01')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3. Multi-core processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For better throughput, we create a pool of workers that can process the years simultaneously. \n",
    "\n",
    "On Cheyenne, these commands can be used to create a pool of 10 DAV workers: \n",
    "\n",
    "```\n",
    "execdav -m 100G -t 8:00:00 -n 1 --cpus-per-task=10\n",
    "\n",
    "export OMP_NUM_THREADS=10\n",
    "\n",
    "start_jupyterLab.sh\n",
    "```\n",
    "\n",
    "Note that I've struggled quite a lot to get this working. It only works when the input datasets are created new each time a worker process is started (function do_work). Defining a global definition of the input data (dictionary `var_dict`) didn't work nicely together with `multiprocessing.Pool`, see my post here: https://github.com/pydata/xarray/issues/3781"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"has multiple fill values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_work(year):\n",
    "    \"\"\"\n",
    "    Worker function to process the data in parallel. \n",
    "    \n",
    "    UPDATE: now with private var_dict because of HDF5 errors (Dask + multiprocessing don't like each other)\n",
    "    \"\"\"\n",
    "    print('processing ', year)\n",
    "    \n",
    "    var_dict = {}\n",
    "    read_ATM(var_dict)\n",
    "    lat, lon = var_dict['hus'].lat, var_dict['hus'].lon\n",
    "    read_oceanvar(var_dict, 'siconc', lat, lon)\n",
    "    read_oceanvar(var_dict, 'tos', lat, lon)\n",
    "\n",
    "#     for varname in var_dict.keys():\n",
    "#         print(varname, var_dict[varname].shape)  \n",
    "    \n",
    "    for month in range(1,13):\n",
    "        mon_str = f'{year}-{month:02d}'\n",
    "        ds = get_DataSet_month(var_dict, mon_str)\n",
    "        set_global_attributes(ds)\n",
    "#         print(mon_str)\n",
    "        ds.to_netcdf(f'monthly/{mon_str}.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running 1 cores\n"
     ]
    }
   ],
   "source": [
    "print('running', NPROC, 'cores')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing  2011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/glade/u/home/lvank/miniconda3/envs/base2/lib/python3.8/site-packages/xarray/conventions.py:492: SerializationWarning: variable 'hus' has multiple fill values {1e+20, 1e+20}, decoding all values to NaN.\n",
      "  new_vars[k] = decode_cf_variable(\n",
      "/glade/u/home/lvank/miniconda3/envs/base2/lib/python3.8/site-packages/xarray/conventions.py:492: SerializationWarning: variable 'ta' has multiple fill values {1e+20, 1e+20}, decoding all values to NaN.\n",
      "  new_vars[k] = decode_cf_variable(\n",
      "/glade/u/home/lvank/miniconda3/envs/base2/lib/python3.8/site-packages/xarray/conventions.py:492: SerializationWarning: variable 'ua' has multiple fill values {1e+20, 1e+20}, decoding all values to NaN.\n",
      "  new_vars[k] = decode_cf_variable(\n",
      "/glade/u/home/lvank/miniconda3/envs/base2/lib/python3.8/site-packages/xarray/conventions.py:492: SerializationWarning: variable 'va' has multiple fill values {1e+20, 1e+20}, decoding all values to NaN.\n",
      "  new_vars[k] = decode_cf_variable(\n",
      "/glade/u/home/lvank/miniconda3/envs/base2/lib/python3.8/site-packages/xarray/conventions.py:492: SerializationWarning: variable 'ps' has multiple fill values {1e+20, 1e+20}, decoding all values to NaN.\n",
      "  new_vars[k] = decode_cf_variable(\n",
      "/glade/u/home/lvank/miniconda3/envs/base2/lib/python3.8/site-packages/xarray/conventions.py:492: SerializationWarning: variable 'tas' has multiple fill values {1e+20, 1e+20}, decoding all values to NaN.\n",
      "  new_vars[k] = decode_cf_variable(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing  2013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/glade/u/home/lvank/miniconda3/envs/base2/lib/python3.8/site-packages/xarray/conventions.py:492: SerializationWarning: variable 'hus' has multiple fill values {1e+20, 1e+20}, decoding all values to NaN.\n",
      "  new_vars[k] = decode_cf_variable(\n",
      "/glade/u/home/lvank/miniconda3/envs/base2/lib/python3.8/site-packages/xarray/conventions.py:492: SerializationWarning: variable 'ta' has multiple fill values {1e+20, 1e+20}, decoding all values to NaN.\n",
      "  new_vars[k] = decode_cf_variable(\n",
      "/glade/u/home/lvank/miniconda3/envs/base2/lib/python3.8/site-packages/xarray/conventions.py:492: SerializationWarning: variable 'ua' has multiple fill values {1e+20, 1e+20}, decoding all values to NaN.\n",
      "  new_vars[k] = decode_cf_variable(\n",
      "/glade/u/home/lvank/miniconda3/envs/base2/lib/python3.8/site-packages/xarray/conventions.py:492: SerializationWarning: variable 'va' has multiple fill values {1e+20, 1e+20}, decoding all values to NaN.\n",
      "  new_vars[k] = decode_cf_variable(\n",
      "/glade/u/home/lvank/miniconda3/envs/base2/lib/python3.8/site-packages/xarray/conventions.py:492: SerializationWarning: variable 'ps' has multiple fill values {1e+20, 1e+20}, decoding all values to NaN.\n",
      "  new_vars[k] = decode_cf_variable(\n",
      "/glade/u/home/lvank/miniconda3/envs/base2/lib/python3.8/site-packages/xarray/conventions.py:492: SerializationWarning: variable 'tas' has multiple fill values {1e+20, 1e+20}, decoding all values to NaN.\n",
      "  new_vars[k] = decode_cf_variable(\n"
     ]
    }
   ],
   "source": [
    "#years = range(1950,2015)\n",
    "years = [2011, 2013]\n",
    "with Pool(processes=NPROC) as pool:\n",
    "    pool.map(do_work, years)\n",
    "    #pool.map(do_cpu_work, foo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# q = queue.Queue()\n",
    "\n",
    "\n",
    "# def worker():\n",
    "#     while True:\n",
    "#         item = q.get()\n",
    "#         print(f'Working on {item}')\n",
    "        \n",
    "#         ds = get_DataSet_month(mon_str)\n",
    "#         set_global_attributes(ds)\n",
    "#         ds.to_netcdf(f'monthly/{mon_str}.nc')\n",
    "        \n",
    "#         print(f'Finished {item}')\n",
    "#         q.task_done()\n",
    "\n",
    "# # turn-on the worker thread\n",
    "# threading.Thread(target=worker, daemon=True).start()\n",
    "\n",
    "# # send thirty task requests to the worker\n",
    "# for year in range(2015,2016):\n",
    "#     for month in range(1,13):\n",
    "#         q.put(f'{year}-{month:02d}')\n",
    "        \n",
    "# print('All task requests sent\\n', end='')\n",
    "\n",
    "# # block until all tasks are done\n",
    "# q.join()\n",
    "# print('All work completed')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
